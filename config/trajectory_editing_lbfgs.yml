task: trajectory_editing
num_ode_steps: 10
gradient_checkpoint: False
dno:
  num_opt_steps: 100
  optimizer: LBFGS # Adam, SGD, LBFGS, LevenbergMarquardt, ...
  lr: 1 # LBFGS needs much higher learning rate
  lr_warm_up_steps: 3
  lr_decay_steps: 0 # Set to 0 to disable, -1 to use default (== num_opt_steps)
  lbfgs:
    history_size: 100
    line_search_fn: strong_wolfe # Or "strong_wolfe"
